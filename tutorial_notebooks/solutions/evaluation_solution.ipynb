{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "evaluation_solution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "UztYrYPgPP_q"
      },
      "source": [
        "# 3. Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsW6-UY_PP_t"
      },
      "source": [
        "Welcome to the third notebook of our six part series part of our tutorial on Deep Learning for Human Activity Recognition. This is an optional notebook and will not be covered as part of the live tutorial. Feel free to work through this notebook on your own at home. You can also skip this tutorial if you are already familiar with the evaluation metrics discussed within this notebook. Within the last notebook you learned:\n",
        "\n",
        "- What data cleaning steps usually need to be performed on a raw sensor dataset?\n",
        "- What importance does sensor orientation have? How do we match the sensor orientation of different sensors?\n",
        "- How and why do we perform normalization?\n",
        "- What is a sliding window? How do we apply it?\n",
        "\n",
        "This notebook will give you an overview of the state-of-the-art evaluation metrics applied within the field of HAR, but also Machine and Deep Learning in general. Using a common set of evaluation metrics is an important concept in order to judge the predictive performance of methods, but also be able to compare different methods with each other. Using a toy example, you will be introduced to all evaluation metrics encountered within the DL-ARC pipeline. Of course this set of evaluation metrics is not complete and there exist multiple other ones, nevertheless, we identified these evaluation metrics to be the most popular ones. \n",
        "\n",
        "\n",
        "After completing this notebook you will be answer the following questions:\n",
        "- What are common evaluation metrics to evaluate the predicitive performance of a pipeline? How do we compute them?\n",
        "- What is the confusion matrix? How to read it?\n",
        "\n",
        "\n",
        "**Note:** Parts of this notebook are inspired by [[1]](#1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtu393_gcCqN"
      },
      "source": [
        "## 3.1. Important Remarks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mw197x5ScEe2"
      },
      "source": [
        "If you are accessing this tutorial via [Google Colab](https://colab.research.google.com/github/mariusbock/dl-for-har/blob/main/tutorial_notebooks/training.ipynb), first make sure to use Google Colab in English. This will help us to better assist you with issues that might arise during the tutorial. There are two ways to change the default language if it isn't English already:\n",
        "1. On Google Colab, go to `Help` -> `View in English` \n",
        "2. Change the default language of your browser to `English`.\n",
        "\n",
        "To also ease the communication when communicating errors, enable line numbers within the settings of Colab.\n",
        "\n",
        "1. On Google Colab, go to `Tools` -> `Settings` -> `Editor` -> `Show line numbers`\n",
        "\n",
        "In general, we strongly advise you to use Google Colab as it provides you with a working Python distribution as well as free GPU resources. To make Colab use GPUs, you need to change the current notebooks runtime type via:\n",
        "\n",
        "- `Runtime` -> `Change runtime type` -> `Dropdown` -> `GPU` -> `Save`\n",
        "\n",
        "**Hint:** you can auto-complete code in Colab via `ctrl` + `spacebar`\n",
        "\n",
        "For the live tutorial, we require all participants to use Colab. If you decide to rerun the tutorial at later points and rather want to have it run locally on your machine, feel free to clone our [GitHub repository](https://github.com/mariusbock/dl-for-har)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVrGGSG1PP_t"
      },
      "source": [
        "## 3.2. Toy example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C4AONhPP_u"
      },
      "source": [
        "For simplicity reasons, we will not work with RWHAR dataset within this notebook, but rather use a toy example. \n",
        "\n",
        "Given three subjects which performed a set of 4 activities. Given that we already windowed the data and decided on a label for each window, each subject has a corresponding true label array. We call this the **ground truth** information (see `lines 3-5`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0D6L9wBPP_u"
      },
      "source": [
        "labels = [0, 1, 2, 3]\n",
        "\n",
        "gt_sbj1 = [1, 1, 1, 0, 0, 0, 2, 3, 3, 2]\n",
        "gt_sbj2 = [0, 0, 1, 0, 3, 3, 0, 0, 1, 2]\n",
        "gt_sbj3 = [3, 3, 3, 0, 0, 1, 1, 2, 2, 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-yMDGoiPP_v"
      },
      "source": [
        "Assume that we trained a Deep Learning network on the data of the first subject and evaluated it on the other two subjects. Applying the network now again on the data of each subject will return us three prediction arrays. We call said arrays the **train predicitions** (for subject 1) and **validation predictions** (for subject 2 and 3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U39IGZffPP_w"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# these arrays contain the predictions for each subject made by your \"network\" \n",
        "pred_sbj1 = [1, 1, 0, 0, 2, 2, 2, 3, 3, 1]\n",
        "pred_sbj2 = [0, 1, 1, 1, 3, 2, 1, 1, 0, 2]\n",
        "pred_sbj3 = [0, 2, 3, 0, 1, 1, 1, 3, 0, 2]\n",
        "\n",
        "# we use subject 1 to be our training data, i.e. the train ground truth and predictions\n",
        "train_gt = gt_sbj1\n",
        "train_pred = pred_sbj1\n",
        "\n",
        "# we use subject 2 and 3 to be our validation data, i.e. the validation ground truth and predictions\n",
        "# we therefore concatenate both the data of subject 2 and 3\n",
        "val_gt = np.concatenate((gt_sbj2, gt_sbj3))\n",
        "val_pred = np.concatenate((pred_sbj2, pred_sbj3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hFcDu-yPP_w"
      },
      "source": [
        "We now have everything we need to calculate the evaluation metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3zJlyfbPP_x"
      },
      "source": [
        "## 3.2. Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LStJHO1LPP_x"
      },
      "source": [
        "Classification accuracy is the ratio of the number of correct preditions out of all predictions that your algorithm made. It is therefore calculated as:\n",
        "\n",
        "$$\\text{accuracy} = \\frac{\\text{number of true preditions}}{\\text{number total preditions}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkZJiCzIPP_y"
      },
      "source": [
        "### Task 1: Implement the accuracy evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgVpjG0wPP_y"
      },
      "source": [
        "1. Implement the `accuracy_metric` method which calculates the accuracy as defined above. (`lines 1-9`)\n",
        "2. Calculate the per-subject, train and validation accuracy using the above defined arrays as input. (`lines 11-24`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQpBcdKPP_y"
      },
      "source": [
        "def accuracy_metric(actual, predicted):\n",
        "    # hints: count the number of correct predicitons\n",
        "    # write a for loop which iterates over the index of the the actual or predicited array\n",
        "    # if at a certain index actual==predicted, then increase the correct count\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predicted[i]:\n",
        "            correct += 1\n",
        "    return correct / float(len(actual)) * 100.0\n",
        "\n",
        "print('\\nSubject 1 accuracy:')\n",
        "print(accuracy_metric(gt_sbj1, pred_sbj1))\n",
        "\n",
        "print('\\nSubject 2 accuracy:')\n",
        "print(accuracy_metric(gt_sbj2, pred_sbj2))\n",
        "\n",
        "print('\\nSubject 3 accuracy:')\n",
        "print(accuracy_metric(gt_sbj3, pred_sbj3))\n",
        "\n",
        "print('\\nTraining accuracy:')\n",
        "print(accuracy_metric(train_gt, train_pred))\n",
        "\n",
        "print('\\nValidation accuracy:')\n",
        "print(accuracy_metric(val_gt, val_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jITWeQhhPP_z"
      },
      "source": [
        "## 3.3. True Positives, False Postives, True Negatives and False Negatives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LflVh2v1PP_z"
      },
      "source": [
        "In order to compute the precision, recall and f1-score metrics, we need to first talk about the four types one can classify the outcome of a classification. This is the concept of **true positives, false positives, true negatives and false negatives**. Given a prediction and its ground truth label, the outcome of the prediction for a label i can either be a:\n",
        "\n",
        "- **True Positive (TP)**: the prediction and ground truth label are both of label i.\n",
        "- **False Positive (FP)**: the predicition is falsely of prediction i, even though the ground truth label is some other label.\n",
        "- **False Negative (TN)**: the predicition is not of prediction i, even though the ground truth label is of label i.\n",
        "- **True Negative (FN)**: the prediction and ground truth label are both not of label i.\n",
        "\n",
        "For a multiclass problem, this leaves us for each label with a count of TP, FP, TN and FN. In the following we predefined a function for you which, given a label, a prediction and a ground truth array, calculates the numer of TP, FP, TN and FN. We also calculated the TP, FP, TN and FN values for each of the four labels of our toy example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOKtLPu8PP_0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def compute_tp_tn_fn_fp(y_act, y_pred, label):\n",
        "    y_act = pd.Series(y_act)\n",
        "    y_pred = pd.Series(y_pred)\n",
        "    tp = sum((y_act == label) & (y_pred == label))\n",
        "    tn = sum((y_act != label) & (y_pred != label))\n",
        "    fn = sum((y_act == label) & (y_pred != label))\n",
        "    fp = sum((y_act != label) & (y_pred == label))\n",
        "    return tp, tn, fp, fn\n",
        "\n",
        "tp_0, tn_0, fp_0, fn_0 = compute_tp_tn_fn_fp(gt_sbj1, pred_sbj1, 0)\n",
        "tp_1, tn_1, fp_1, fn_1 = compute_tp_tn_fn_fp(gt_sbj1, pred_sbj1, 1)\n",
        "tp_2, tn_2, fp_2, fn_2 = compute_tp_tn_fn_fp(gt_sbj1, pred_sbj1, 2)\n",
        "tp_3, tn_3, fp_3, fn_3 = compute_tp_tn_fn_fp(gt_sbj1, pred_sbj1, 3)\n",
        "\n",
        "print('\\nSubject 1 TP, FP, TN and FN:')\n",
        "print('\\nLabel 0:')\n",
        "print('True Positives: {0}, False Positives: {1}, True Negatives: {2}, False Negatives: {3}'.format(tp_0, tn_0, fp_0, fn_0))\n",
        "print('\\nLabel 1:')\n",
        "print('True Positives: {0}, False Positives: {1}, True Negatives: {2}, False Negatives: {3}'.format(tp_1, tn_1, fp_1, fn_1))\n",
        "print('\\nLabel 2:')\n",
        "print('True Positives: {0}, False Positives: {1}, True Negatives: {2}, False Negatives: {3}'.format(tp_2, tn_2, fp_2, fn_2))\n",
        "print('\\nLabel 3:')\n",
        "print('True Positives: {0}, False Positives: {1}, True Negatives: {2}, False Negatives: {3}'.format(tp_3, tn_3, fp_3, fn_3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0SF8bDbPP_0"
      },
      "source": [
        "## 3.3. Precision, Recall and F1-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6m5hP8GPP_0"
      },
      "source": [
        "Precision, Recall and F1-score are the most central metrics when it comes classification problems. Each of them assesses a differnt viewpoint of how your algorithm solved the classification problem for a specific label. In the following we will go over how to compute each metric, compute them for each label and take the average across labels to obtain a final assessment for our toy example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1FrtZYyPP_0"
      },
      "source": [
        "### 3.3.1. Precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWibL_zkPP_1"
      },
      "source": [
        "For a class label $i$, precision is the fraction of correct predictions for the label at hand out of all instances where the algorithm predicted the label to be $i$. For a class label $i$ it is computed as:\n",
        "\n",
        "$$P_i = \\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FP}_i} \\text{ for given class } i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RQPOJJdPP_1"
      },
      "source": [
        "#### Task 2: Implement the precision evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHa2MLSpPP_1"
      },
      "source": [
        "1. Implement the `precision_metric` method which calculates the precision for a set of labels as defined above. (`lines 1-16`)\n",
        "2. Calculate the per-subject, train and validation precision using the above defined arrays as input. (`lines 18-31`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ix0gNXexPP_1"
      },
      "source": [
        "def precision(actual, predicted, labels):\n",
        "    # counter variable to accumulate precision across labels\n",
        "    total_prec = 0\n",
        "    # iterate of the labels\n",
        "    for label in labels:\n",
        "        # calculate the number of tp, fp, tn, fn for said label using the actual and predicted arrays\n",
        "        tp, tn, fp, fn = compute_tp_tn_fn_fp(actual, predicted, label)\n",
        "        print('Precision Label {0}:'.format(label))\n",
        "        # calculate the precision value (hint: multiply by 100 to get nice percentage values)\n",
        "        prec = tp / (tp + fp) * 100\n",
        "        print(prec)\n",
        "        # adds up the precision to the total count\n",
        "        total_prec += prec\n",
        "    # prints the average precision as the unweigthed average across all classes\n",
        "    print('Average Precision:')\n",
        "    print(total_prec / len(labels))\n",
        "    \n",
        "print('\\nSubject 1 precision:')\n",
        "precision(gt_sbj1, pred_sbj1, labels)\n",
        "\n",
        "print('\\nSubject 2 precision:')\n",
        "precision(gt_sbj2, pred_sbj2, labels)\n",
        "\n",
        "print('\\nSubject 3 precision:')\n",
        "precision(gt_sbj3, pred_sbj3, labels)\n",
        "\n",
        "print('\\nTraining precision:')\n",
        "precision(train_gt, train_pred, labels)\n",
        "\n",
        "print('\\nValidation precision:')\n",
        "precision(val_gt, val_pred, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv2C8U7APP_1"
      },
      "source": [
        "### 3.3.2. Recall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOhouRftPP_1"
      },
      "source": [
        "Recall is the fraction of correct predictions for the label at hand over all instances which have the label $i$. For a class label $i$ it is computed as:\n",
        "\n",
        "$$R_i =\\frac{\\text{TP}_i}{\\text{TP}_i + \\text{FN}_i} \\text{ for given class } i$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPNt2ypkPP_2"
      },
      "source": [
        "#### Task 3: Implement the recall evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vunuy-kIPP_2"
      },
      "source": [
        "1. Implement the `recall_metric` method which calculates the precision for a set of labels as defined above. (`lines 1-16`)\n",
        "2. Calculate the per-subject, train and validation precision using the above defined arrays as input. (`lines 18-31`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rECg7-mcPP_2"
      },
      "source": [
        "def recall(actual, predicted, labels):\n",
        "    # counter variable to accumulate recall across labels\n",
        "    total_rec = 0\n",
        "    # iterate of the labels\n",
        "    for label in labels:\n",
        "        # calculate the number of tp, fp, tn, fn for said label using the actual and predicted arrays\n",
        "        tp, tn, fp, fn = compute_tp_tn_fn_fp(actual, predicted, label)\n",
        "        print('Recall Label {0}:'.format(label))\n",
        "        # calculate the recall value (hint: multiply by 100 to get nice percentage values)\n",
        "        rec = tp / (tp + fn) * 100\n",
        "        print(rec)\n",
        "        # adds up the recall to the total count\n",
        "        total_rec += rec\n",
        "    print('Average Recall:')\n",
        "    # prints the average recall as the unweigthed average across all classes\n",
        "    print(total_rec / len(labels))\n",
        "    \n",
        "print('\\nSubject 1 recall:')\n",
        "recall(gt_sbj1, pred_sbj1, labels)\n",
        "\n",
        "print('\\nSubject 2 recall:')\n",
        "recall(gt_sbj2, pred_sbj2, labels)\n",
        "\n",
        "print('\\nSubject 3 recall:')\n",
        "recall(gt_sbj3, pred_sbj3, labels)\n",
        "\n",
        "print('\\nTraining recall:')\n",
        "recall(train_gt, train_pred, labels)\n",
        "\n",
        "print('\\nValidation recall:')\n",
        "recall(val_gt, val_pred, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koyWZ5RSPP_2"
      },
      "source": [
        "### 3.3.3. F1-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dhm8okJ3PP_2"
      },
      "source": [
        "The F1-score combines both recall and precision into one measure. It is computed as the harmonic mean of precision and recall, i.e.:\n",
        "\n",
        "$$F_i = 2 * \\frac{P_i * R_i}{P_i + R_i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_bXjypPPP_2"
      },
      "source": [
        "#### Task 3: Implement the F1-score evaluation metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxvOTWBAPP_2"
      },
      "source": [
        "1. Implement the `f1_score_metric` method which calculates the precision for a set of labels as defined above. (`lines 1-16`)\n",
        "2. Calculate the per-subject, train and validation precision using the above defined arrays as input. (`lines 18-31`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl6OFLBGPP_2"
      },
      "source": [
        "def f1_score_metric(actual, predicted, labels):\n",
        "    # counter variable to accumulate f1-score across labels\n",
        "    total_f1 = 0\n",
        "    # iterate of the labels\n",
        "    for label in labels:\n",
        "        # calculate the number of tp, fp, tn, fn for said label using the actual and predicted arrays\n",
        "        tp, tn, fp, fn = compute_tp_tn_fn_fp(actual, predicted, label)\n",
        "        print('F1-score Label {0}:'.format(label))\n",
        "        # calculate the recall and precision value\n",
        "        rec = tp / (tp + fn)\n",
        "        prec = tp / (tp + fp)\n",
        "        # define the counter and denominator of the f1-score formula above\n",
        "        counter = prec * rec\n",
        "        denominator = prec + rec\n",
        "        # caluclate the f1 score for the given label\n",
        "        f1 = 2 * (counter / denominator) * 100\n",
        "        print(f1)\n",
        "        # adds up the f1-score to the total count\n",
        "        total_f1 += f1\n",
        "    print('Average F1-score:')\n",
        "    # prints the average f1-score as the unweigthed average across all classes\n",
        "    print(total_f1 / len(labels))\n",
        "    \n",
        "print('\\nSubject 1 F1-score:')\n",
        "f1_score_metric(gt_sbj1, pred_sbj1, labels)\n",
        "\n",
        "print('\\nSubject 2 F1-score:')\n",
        "f1_score_metric(gt_sbj2, pred_sbj2, labels)\n",
        "\n",
        "print('\\nSubject 3 F1-score:')\n",
        "f1_score_metric(gt_sbj3, pred_sbj3, labels)\n",
        "\n",
        "print('\\nTraining F1-score:')\n",
        "f1_score_metric(train_gt, train_pred, labels)\n",
        "\n",
        "print('\\nValidation F1-score:')\n",
        "f1_score_metric(val_gt, val_pred, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miMrLVoOPP_3"
      },
      "source": [
        "## 3.3. Confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nB_qlJ28PP_3"
      },
      "source": [
        "A confusion matrix gives you a tabular overview of all predicitions compared to the ground truth values. The counts of actual labels are oriented horizontally, i.e. column-wise. The counts of predicted labels are oriented vertically, i.e. row-wise. A perfect confusion matrix would be a diagonal matrix, i.e. all labels are correctly predicted and have the same predicted label and ground truth label.\n",
        "\n",
        "You can use a confusion matrix to quickly spot which classes your algorithm might have struggled with the most."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwqP3aSTpOqj"
      },
      "source": [
        "### Task 5: Interpreting the confusion matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQCD3eA1pYt4"
      },
      "source": [
        "1. Run the code below. How can you interpret the results? Do you spot any activities/ subjects which your algorithm paticularly struggeled with? Which activities/ subjects did you algorithm perfom well on? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liM7H41gPP_3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import time \n",
        "\n",
        "\n",
        "def plot_confusion_matrix(actual, predicted, labels):\n",
        "    cm = confusion_matrix(actual, predicted, labels=labels)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "    disp.plot() \n",
        "\n",
        "print('\\Confusion Matrix Subject 1:')\n",
        "plot_confusion_matrix(gt_sbj1, pred_sbj1, labels)\n",
        "\n",
        "print('\\Confusion Matrix Subject 2:')\n",
        "plot_confusion_matrix(gt_sbj2, pred_sbj2, labels)\n",
        "\n",
        "print('\\Confusion Matrix Subject 3:')\n",
        "plot_confusion_matrix(gt_sbj3, pred_sbj3, labels)\n",
        "\n",
        "print('\\Confusion Matrix Training:')\n",
        "plot_confusion_matrix(train_gt, train_pred, labels)\n",
        "\n",
        "print('\\Confusion Matrix Validation:')\n",
        "plot_confusion_matrix(val_gt, val_pred, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLfIjvskaJz3"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q2EksJ9aL6T"
      },
      "source": [
        "<a id=\"1\">[1]</a>   Jason Brownlee. 2016. How To Implement Machine Learning Metrics From Scratch in Python. In Code Algorithms From Scratch. https://machinelearningmastery.com/implement-machine-learning-algorithm-performance-metrics-scratch-python/\n",
        "\n"
      ]
    }
  ]
}
